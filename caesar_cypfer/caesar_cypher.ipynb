{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"caesar_cypher.ipynb\"","provenance":[{"file_id":"1OIkRLAuy0lMHt8dbDmPGoaDmDoe3Uk-V","timestamp":1613992332751}],"collapsed_sections":["tEgtmsLH-jhy","9sE_4qUwFkcK","2DtLfFDJIrxM","un1Jv5TxNxlg","6MQvU-O7POt0","pthANfxtQQgD"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sa0qBTJwzKeG"},"source":["### **Основное задание**\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"KLi1lZJJzUIW"},"source":["Задание 1.  \r\n","Обучите нейронную сеть решать шифр цезаря.  \r\n","Что надо сделать:  \r\n","1.Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и т.п.)  \r\n","2.Сделать нейронную сеть.  \r\n","3.Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза).  \r\n","4.Проверить качество.  \r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"33rP1XXKeuYm"},"source":["# Загружаем библиотеки. Смотрим, что доступно cpu или cuda. Загружаем данные.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAyHNSz03Ks4","executionInfo":{"status":"ok","timestamp":1614076346191,"user_tz":-180,"elapsed":39398,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"96c0fb78-3410-4869-90dd-ebe5eed5f0f8"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"21v9fScZjLo-","executionInfo":{"status":"ok","timestamp":1614076354397,"user_tz":-180,"elapsed":3800,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["import torch\r\n","import random\r\n","import time"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"GISwEoRNz7WG","executionInfo":{"status":"ok","timestamp":1614076357340,"user_tz":-180,"elapsed":1257,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MRb8_PJz7YJ","executionInfo":{"status":"ok","timestamp":1614076359535,"user_tz":-180,"elapsed":1491,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"75bb3797-b9ea-4019-e89a-8c7a8592078c"},"source":["dev"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"5u5w9Ygtz7o3","executionInfo":{"status":"ok","timestamp":1614076364263,"user_tz":-180,"elapsed":1163,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["data_dir = '/content/drive/My Drive/Colab Notebooks/caesar_cypher/'\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYNZTl3P44kA"},"source":["# **1. Написать алгоритм шифра цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и т.п.)**"]},{"cell_type":"markdown","metadata":{"id":"fo-AEtyVZ5C8"},"source":["Создадим функцию, которая сдвигает буквы согласно алгоритму шифра цезаря."]},{"cell_type":"code","metadata":{"id":"8qbM3xv6YtF1","executionInfo":{"status":"ok","timestamp":1614076371407,"user_tz":-180,"elapsed":1326,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["key = 11\r\n","vocab = [char for char in 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя']\r\n","\r\n","\r\n","def encrypt(text):\r\n","    indexes = [vocab.index(char) for char in text]\r\n","    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\r\n","    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\r\n","    encrypted = ''.join(encrypted_chars)\r\n","    return encrypted"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_pgnnT4aGw9"},"source":["Применим нашу функцию"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOVMIJHGYuZ1","executionInfo":{"status":"ok","timestamp":1614076375595,"user_tz":-180,"elapsed":772,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"a6527e5a-8cb0-46f0-a358-f87a127757dd"},"source":["print(encrypt('абвгдеёжзийклмнопрстуфхцчшщъыьэюя'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["клмнопрстуфхцчшщъыьэюяабвгдеёжзий\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VsHXwIriaPFh"},"source":["Создадим датасет со случайным выбором букв."]},{"cell_type":"code","metadata":{"id":"zBrJWk2sY_dQ","executionInfo":{"status":"ok","timestamp":1614076385843,"user_tz":-180,"elapsed":601,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["num_examples = 128\r\n","message_length = 32\r\n","\r\n","\r\n","def dataset(num_examples):\r\n","    dataset = []\r\n","    for x in range(num_examples):\r\n","        ex_out = ''.join([random.choice(vocab) for x in range(message_length)])\r\n","        ex_in = encrypt(''.join(ex_out))\r\n","        ex_in = [vocab.index(x) for x in ex_in]\r\n","        ex_out = [vocab.index(x) for x in ex_out]\r\n","        dataset.append([torch.tensor(ex_in), torch.tensor(ex_out)])\r\n","    return dataset"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8ofIUtk4-F6"},"source":["## **RNN**"]},{"cell_type":"code","metadata":{"id":"iH_9DeGd4ZVG","executionInfo":{"status":"ok","timestamp":1614076391159,"user_tz":-180,"elapsed":583,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["embedding_dim = 10\r\n","hidden_dim = 10\r\n","vocab_size = len(vocab)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISmWV3pkhQg8","executionInfo":{"status":"ok","timestamp":1614076393695,"user_tz":-180,"elapsed":690,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["class Network(torch.nn.Module):\r\n","    def __init__(self):\r\n","        super(Network, self).__init__()\r\n","        ## Здесь создать слои\r\n","        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\r\n","        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\r\n","        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\r\n","        \r\n","    def forward(self, sentences, state=None):\r\n","        ## Здесь применить\r\n","        embed = self.embed(sentences)\r\n","        o, s = self.rnn(embed)\r\n","        out = self.linear(o)\r\n","        return out"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUeLFkWQoowW","executionInfo":{"status":"ok","timestamp":1614076401773,"user_tz":-180,"elapsed":584,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["model = Network ()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"sb26I1fhscYv","executionInfo":{"status":"ok","timestamp":1614076403723,"user_tz":-180,"elapsed":588,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["criterion = torch.nn.CrossEntropyLoss()\r\n","optimizer = torch.optim.SGD(model.parameters(), lr=.05)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"rs8MYyvLd6hz"},"source":["Обучение:"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMp9evAypj8o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614076409828,"user_tz":-180,"elapsed":1975,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"9e02d360-0859-4739-9209-9653d6dc1e0c"},"source":["for ep in range(10):\r\n","    start = time.time()\r\n","    train_loss = 0.\r\n","    train_passed = 0\r\n","\r\n","    for encrypted, original in dataset(num_examples):\r\n","        \r\n","        optimizer.zero_grad()\r\n","        answers = model.forward(encrypted.unsqueeze(1))\r\n","        answers = answers.view(-1, vocab_size)\r\n","        loss = criterion(answers, original)\r\n","        train_loss += loss.item()\r\n","\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        train_passed += 1\r\n","\r\n","    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 0. Time: 0.265, Train loss: 3.106\n","Epoch 1. Time: 0.137, Train loss: 2.386\n","Epoch 2. Time: 0.139, Train loss: 1.803\n","Epoch 3. Time: 0.130, Train loss: 1.343\n","Epoch 4. Time: 0.148, Train loss: 1.015\n","Epoch 5. Time: 0.134, Train loss: 0.791\n","Epoch 6. Time: 0.145, Train loss: 0.636\n","Epoch 7. Time: 0.136, Train loss: 0.526\n","Epoch 8. Time: 0.144, Train loss: 0.441\n","Epoch 9. Time: 0.134, Train loss: 0.374\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g-EL5P6Hd34G"},"source":["Проверка качества:"]},{"cell_type":"code","metadata":{"id":"9r0kSmYu-g9U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614076427755,"user_tz":-180,"elapsed":617,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"7acf6351-d481-487e-df27-e4bfa6d64940"},"source":["with torch.no_grad():\r\n","        matches, total = 0, 0\r\n","        for encrypted, original in dataset(num_examples):\r\n","            answers = model.forward(encrypted.unsqueeze(1))\r\n","            predictions = torch.nn.functional.softmax(answers, dim=2)\r\n","            _, batch_out = predictions.max(dim=2)\r\n","            batch_out = batch_out.squeeze(1)\r\n","            matches += torch.eq(batch_out, original).sum().item()\r\n","            total += torch.numel(batch_out)\r\n","        accuracy = matches / total\r\n","        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Accuracy: 96.88%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tEgtmsLH-jhy"},"source":["## **LSTM**"]},{"cell_type":"code","metadata":{"id":"hSBo1cWY-y9n","executionInfo":{"status":"ok","timestamp":1614076454679,"user_tz":-180,"elapsed":592,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["embedding_dim = 10\r\n","hidden_dim = 10\r\n","vocab_size = len(vocab)\r\n","\r\n","embed = torch.nn.Embedding(vocab_size, embedding_dim)\r\n","lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\r\n","linear = torch.nn.Linear(hidden_dim, vocab_size)\r\n","softmax = torch.nn.functional.softmax\r\n","loss_fn = torch.nn.CrossEntropyLoss()\r\n","optimizer = torch.optim.Adam(list(embed.parameters()) +\r\n","                             list(lstm.parameters()) +\r\n","                             list(linear.parameters()), lr=0.001)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nj_5EZn-y-2","executionInfo":{"status":"ok","timestamp":1614076457394,"user_tz":-180,"elapsed":592,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["def zero_hidden():\r\n","    return (torch.zeros(1, 1, hidden_dim),\r\n","            torch.zeros(1, 1, hidden_dim))"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2o2pXBWedr2J"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"pwzKsG6wdr4o"},"source":["Обучение:"]},{"cell_type":"code","metadata":{"id":"k0dy_If3-zAK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614076475991,"user_tz":-180,"elapsed":10494,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"db0b3156-0e4d-4121-ae83-b5ffba7f5304"},"source":["num_epochs = 10\r\n","\r\n","accuracies, max_accuracy = [], 0\r\n","for x in range(num_epochs):\r\n","    print('Epoch: {}'.format(x))\r\n","    for encrypted, original in dataset(num_examples):\r\n","        lstm_in = embed(encrypted)\r\n","        lstm_in = lstm_in.unsqueeze(1)\r\n","        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\r\n","        scores = linear(lstm_out)\r\n","        scores = scores.transpose(1, 2)\r\n","        original = original.unsqueeze(1)\r\n","        loss = loss_fn(scores, original) \r\n","        loss.backward()\r\n","        optimizer.step()\r\n","    print('Loss: {:6.4f}'.format(loss.item()))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Epoch: 0\n","Loss: 3.1223\n","Epoch: 1\n","Loss: 1.9948\n","Epoch: 2\n","Loss: 1.0568\n","Epoch: 3\n","Loss: 0.5020\n","Epoch: 4\n","Loss: 0.2941\n","Epoch: 5\n","Loss: 0.1096\n","Epoch: 6\n","Loss: 0.0940\n","Epoch: 7\n","Loss: 0.0564\n","Epoch: 8\n","Loss: 0.0407\n","Epoch: 9\n","Loss: 0.0177\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Un1mfqAtdxmj"},"source":["Проверка качества:"]},{"cell_type":"code","metadata":{"id":"RFYPbUQ-_htj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614076537274,"user_tz":-180,"elapsed":588,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"667f025b-7536-4ae8-90d8-82bdac2c9ac7"},"source":["with torch.no_grad():\r\n","        matches, total = 0, 0\r\n","        for encrypted, original in dataset(num_examples):\r\n","            lstm_in = embed(encrypted)\r\n","            lstm_in = lstm_in.unsqueeze(1)\r\n","            lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\r\n","            scores = linear(lstm_out)\r\n","            predictions = softmax(scores, dim=2)\r\n","            _, batch_out = predictions.max(dim=2)\r\n","            batch_out = batch_out.squeeze(1)\r\n","            matches += torch.eq(batch_out, original).sum().item()\r\n","            total += torch.numel(batch_out)\r\n","        accuracy = matches / total\r\n","        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Accuracy: 100.00%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"id2lgwqqFidK"},"source":["# **Задание 2.**\r\n","# **Выполнить практическую работу из лекционного ноутбука.**  \r\n","# **а) построить RNN-ячейку на основе полносвязных слоев**  \r\n","# **б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”**  "]},{"cell_type":"markdown","metadata":{"id":"9sE_4qUwFkcK"},"source":["## **Загружаем данные** "]},{"cell_type":"code","metadata":{"id":"Cj_6xKpuiUn1","executionInfo":{"status":"ok","timestamp":1614077853113,"user_tz":-180,"elapsed":744,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["import pandas as pd"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"28uOypTPR3EJ","executionInfo":{"status":"ok","timestamp":1614077855633,"user_tz":-180,"elapsed":1085,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["df = pd.read_csv(data_dir+'data.csv')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"OsZsgD4MF1L9","executionInfo":{"status":"ok","timestamp":1614077869323,"user_tz":-180,"elapsed":644,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"e0da48d5-8878-42b8-f63a-6372f25764a5"},"source":["df.head()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>episode_id</th>\n","      <th>number</th>\n","      <th>raw_text</th>\n","      <th>timestamp_in_ms</th>\n","      <th>speaking_line</th>\n","      <th>character_id</th>\n","      <th>location_id</th>\n","      <th>raw_character_text</th>\n","      <th>raw_location_text</th>\n","      <th>spoken_words</th>\n","      <th>normalized_text</th>\n","      <th>word_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>10368</td>\n","      <td>35</td>\n","      <td>29</td>\n","      <td>Lisa Simpson: Maggie, look. What's that?</td>\n","      <td>235000</td>\n","      <td>True</td>\n","      <td>9</td>\n","      <td>5.0</td>\n","      <td>Lisa Simpson</td>\n","      <td>Simpson Home</td>\n","      <td>Maggie, look. What's that?</td>\n","      <td>maggie look whats that</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>10369</td>\n","      <td>35</td>\n","      <td>30</td>\n","      <td>Lisa Simpson: Lee-mur. Lee-mur.</td>\n","      <td>237000</td>\n","      <td>True</td>\n","      <td>9</td>\n","      <td>5.0</td>\n","      <td>Lisa Simpson</td>\n","      <td>Simpson Home</td>\n","      <td>Lee-mur. Lee-mur.</td>\n","      <td>lee-mur lee-mur</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>10370</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>Lisa Simpson: Zee-boo. Zee-boo.</td>\n","      <td>239000</td>\n","      <td>True</td>\n","      <td>9</td>\n","      <td>5.0</td>\n","      <td>Lisa Simpson</td>\n","      <td>Simpson Home</td>\n","      <td>Zee-boo. Zee-boo.</td>\n","      <td>zee-boo zee-boo</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>10372</td>\n","      <td>35</td>\n","      <td>33</td>\n","      <td>Lisa Simpson: I'm trying to teach Maggie that ...</td>\n","      <td>245000</td>\n","      <td>True</td>\n","      <td>9</td>\n","      <td>5.0</td>\n","      <td>Lisa Simpson</td>\n","      <td>Simpson Home</td>\n","      <td>I'm trying to teach Maggie that nature doesn't...</td>\n","      <td>im trying to teach maggie that nature doesnt e...</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>10374</td>\n","      <td>35</td>\n","      <td>35</td>\n","      <td>Lisa Simpson: It's like an ox, only it has a h...</td>\n","      <td>254000</td>\n","      <td>True</td>\n","      <td>9</td>\n","      <td>5.0</td>\n","      <td>Lisa Simpson</td>\n","      <td>Simpson Home</td>\n","      <td>It's like an ox, only it has a hump and a dewl...</td>\n","      <td>its like an ox only it has a hump and a dewlap...</td>\n","      <td>18.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...  word_count\n","0           0  ...         4.0\n","1           1  ...         2.0\n","2           2  ...         2.0\n","3           3  ...        24.0\n","4           4  ...        18.0\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"yHeNk8DLHjt8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614078287419,"user_tz":-180,"elapsed":606,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"c25a4854-e6c1-4aae-d34d-41eb75ef49ae"},"source":["phrases = df['normalized_text'].tolist()\r\n","phrases[:10]"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['maggie look whats that',\n"," 'lee-mur lee-mur',\n"," 'zee-boo zee-boo',\n"," 'im trying to teach maggie that nature doesnt end with the barnyard i want her to have all the advantages that i didnt have',\n"," 'its like an ox only it has a hump and a dewlap hump and dew-lap hump and dew-lap',\n"," 'you know his blood type how romantic',\n"," 'oh yeah whats my shoe size',\n"," 'ring',\n"," 'yes dad',\n"," 'ooh look maggie what is that do-dec-ah-edron dodecahedron']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"wlhTD-kPkC06","executionInfo":{"status":"ok","timestamp":1614078407465,"user_tz":-180,"elapsed":632,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["text = [[c for c in ph] for ph in phrases if type(ph) is str]"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DtLfFDJIrxM"},"source":["\r\n"," ## **Делаем массив с данными**"]},{"cell_type":"code","metadata":{"id":"Y5HrjrbzWaW7","executionInfo":{"status":"ok","timestamp":1614078499607,"user_tz":-180,"elapsed":625,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["CHARS = set('abcdefghijklmnopqrstuvwxyz ')\r\n","INDEX_TO_CHAR = ['none'] + [w for w in CHARS]\r\n","CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN1bkaRrxGEa","executionInfo":{"status":"ok","timestamp":1614081727726,"user_tz":-180,"elapsed":625,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"2c7f4016-0c5b-4f42-9830-642be33a9e3c"},"source":["INDEX_TO_CHAR"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['none',\n"," 'f',\n"," 'n',\n"," 'v',\n"," 'p',\n"," 'u',\n"," 'b',\n"," 'd',\n"," 'k',\n"," 'o',\n"," 'g',\n"," 'h',\n"," 's',\n"," 'y',\n"," 'a',\n"," 'l',\n"," 'z',\n"," 'q',\n"," 'i',\n"," 'j',\n"," 'm',\n"," ' ',\n"," 'c',\n"," 'r',\n"," 't',\n"," 'w',\n"," 'x',\n"," 'e']"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOEOltDUxR65","executionInfo":{"status":"ok","timestamp":1614081817935,"user_tz":-180,"elapsed":594,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"1c739731-9078-4a09-fd22-2047d8736bb0"},"source":["CHAR_TO_INDEX"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ': 21,\n"," 'a': 14,\n"," 'b': 6,\n"," 'c': 22,\n"," 'd': 7,\n"," 'e': 27,\n"," 'f': 1,\n"," 'g': 10,\n"," 'h': 11,\n"," 'i': 18,\n"," 'j': 19,\n"," 'k': 8,\n"," 'l': 15,\n"," 'm': 20,\n"," 'n': 2,\n"," 'none': 0,\n"," 'o': 9,\n"," 'p': 4,\n"," 'q': 17,\n"," 'r': 23,\n"," 's': 12,\n"," 't': 24,\n"," 'u': 5,\n"," 'v': 3,\n"," 'w': 25,\n"," 'x': 26,\n"," 'y': 13,\n"," 'z': 16}"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"I5pRkbP2k27M","executionInfo":{"status":"ok","timestamp":1614081850465,"user_tz":-180,"elapsed":3344,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["MAX_LEN = 50\r\n","X = torch.zeros((len(text), MAX_LEN), dtype=int)\r\n","for i in range(len(text)):\r\n","    for j, w in enumerate(text[i]):\r\n","        if j >= MAX_LEN:\r\n","            break\r\n","        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQiZmPICk9Da","executionInfo":{"status":"ok","timestamp":1614081861677,"user_tz":-180,"elapsed":770,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"1504b35e-e85f-4bea-da48-b762d7854ffb"},"source":["X[0:10]"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[20, 14, 10, 10, 18, 27, 21, 15,  9,  9,  8, 21, 25, 11, 14, 24, 12, 21,\n","         24, 11, 14, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [15, 27, 27,  0, 20,  5, 23, 21, 15, 27, 27,  0, 20,  5, 23,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [16, 27, 27,  0,  6,  9,  9, 21, 16, 27, 27,  0,  6,  9,  9,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [18, 20, 21, 24, 23, 13, 18,  2, 10, 21, 24,  9, 21, 24, 27, 14, 22, 11,\n","         21, 20, 14, 10, 10, 18, 27, 21, 24, 11, 14, 24, 21,  2, 14, 24,  5, 23,\n","         27, 21,  7,  9, 27, 12,  2, 24, 21, 27,  2,  7, 21, 25],\n","        [18, 24, 12, 21, 15, 18,  8, 27, 21, 14,  2, 21,  9, 26, 21,  9,  2, 15,\n","         13, 21, 18, 24, 21, 11, 14, 12, 21, 14, 21, 11,  5, 20,  4, 21, 14,  2,\n","          7, 21, 14, 21,  7, 27, 25, 15, 14,  4, 21, 11,  5, 20],\n","        [13,  9,  5, 21,  8,  2,  9, 25, 21, 11, 18, 12, 21,  6, 15,  9,  9,  7,\n","         21, 24, 13,  4, 27, 21, 11,  9, 25, 21, 23,  9, 20, 14,  2, 24, 18, 22,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [ 9, 11, 21, 13, 27, 14, 11, 21, 25, 11, 14, 24, 12, 21, 20, 13, 21, 12,\n","         11,  9, 27, 21, 12, 18, 16, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [23, 18,  2, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [13, 27, 12, 21,  7, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [ 9,  9, 11, 21, 15,  9,  9,  8, 21, 20, 14, 10, 10, 18, 27, 21, 25, 11,\n","         14, 24, 21, 18, 12, 21, 24, 11, 14, 24, 21,  7,  9,  0,  7, 27, 22,  0,\n","         14, 11,  0, 27,  7, 23,  9,  2, 21,  7,  9,  7, 27, 22]])"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"QLaarWRzlIAs"},"source":["## **Смотрим на Embedding и RNN ячейку**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIhDf5sblCfS","executionInfo":{"status":"ok","timestamp":1614078646666,"user_tz":-180,"elapsed":587,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"f64b1481-7247-4747-f693-fb1c2ca0665b"},"source":["embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\r\n","t = embeddings(X[0:10])\r\n","t"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.2240, -1.5190,  0.8191,  ..., -1.0770,  0.7025, -0.6865],\n","         [-0.5465,  1.7345, -0.6493,  ..., -1.0592, -1.5871,  0.3748],\n","         [ 2.1783, -1.2437,  0.2973,  ...,  0.4653, -0.7692,  1.2772],\n","         ...,\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710]],\n","\n","        [[-0.6415, -1.0177, -1.4828,  ...,  0.1927, -1.3682,  0.5728],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         ...,\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710]],\n","\n","        [[-2.0601,  0.5091, -0.5492,  ...,  1.4292,  0.9575,  0.0883],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         ...,\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710]],\n","\n","        ...,\n","\n","        [[ 0.9277,  1.6567,  0.3661,  ..., -0.0378, -1.3776,  1.0192],\n","         [ 0.0560, -0.7834,  0.3073,  ...,  0.4782, -1.0501, -0.0864],\n","         [ 0.3624,  0.8761, -1.0442,  ..., -0.0510, -0.7729,  1.3546],\n","         ...,\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710]],\n","\n","        [[ 0.4983, -0.7267,  0.6875,  ..., -1.1750,  1.3208,  0.6606],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         [-1.5921, -0.0873,  1.5675,  ..., -0.0370,  0.7665,  2.5706],\n","         ...,\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710],\n","         [-0.3687, -0.1730, -0.4970,  ...,  1.4607,  1.5439, -1.0710]],\n","\n","        [[-0.9111,  0.1147,  0.7154,  ...,  0.2692,  0.7177,  0.3769],\n","         [-0.9111,  0.1147,  0.7154,  ...,  0.2692,  0.7177,  0.3769],\n","         [ 0.0853,  0.3941,  0.1848,  ...,  0.2855,  0.2409, -0.2980],\n","         ...,\n","         [ 0.5130,  0.0593,  0.3107,  ..., -0.5722,  1.5822, -2.2152],\n","         [-1.3439, -0.9808,  0.6842,  ...,  0.2097,  0.7494,  0.7906],\n","         [-2.0479, -1.6434,  0.5644,  ...,  0.3587,  3.3083, -1.7346]]],\n","       grad_fn=<EmbeddingBackward>)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQcjstXnlaah","executionInfo":{"status":"ok","timestamp":1614078666812,"user_tz":-180,"elapsed":584,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"e0b079d2-7691-446c-e5ab-579bd20bbdd9"},"source":["t.shape, X[0:10].shape"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([10, 50, 28]), torch.Size([10, 50]))"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwpopTlxlfO_","executionInfo":{"status":"ok","timestamp":1614078685531,"user_tz":-180,"elapsed":592,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"146e2c3c-b10c-43e6-c828-2f245024257e"},"source":["rnn = torch.nn.RNN(28, 128, batch_first=True)\r\n","o, s = rnn(t)\r\n","o.shape, s.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([10, 50, 128]), torch.Size([1, 10, 128]))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3hK4pDPlkdW","executionInfo":{"status":"ok","timestamp":1614078706343,"user_tz":-180,"elapsed":603,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"dbf43d70-42da-4dd5-c35c-393a63d23465"},"source":["o, s2 = rnn(t, s)\r\n","o.shape, s2.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([10, 50, 128]), torch.Size([1, 10, 128]))"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"V4P0HuUBlpJ4"},"source":["## **Практика. Реализуйте код модели нейронной сети.**"]},{"cell_type":"markdown","metadata":{"id":"LoQGeCl8mAVm"},"source":["3 слоя - embeding (28), скрытая ячейка (128), полносвязанный из состояния rnn в букву (28)"]},{"cell_type":"markdown","metadata":{"id":"IzWkAJUqnTfh"},"source":["Был код на лекции (оставляем в качестве примера скелета):"]},{"cell_type":"code","metadata":{"id":"y1Yhlh97l0TJ","executionInfo":{"status":"ok","timestamp":1614081926357,"user_tz":-180,"elapsed":590,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["class Network(torch.nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(Network, self).__init__()\r\n","        self.word_embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\r\n","        self.gru = torch.nn.GRU(28, 128, batch_first=True)\r\n","        self.hidden2tag = torch.nn.Linear(128, len(INDEX_TO_CHAR))\r\n","\r\n","    def forward(self, sentences):\r\n","        embeds = self.word_embeddings(sentences)\r\n","        gru_out, state = self.gru(embeds)\r\n","        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\r\n","        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state\r\n","\r\n","    def forward_state(self, sentences, state):\r\n","        embeds = self.word_embeddings(sentences)\r\n","        gru_out, state = self.gru(embeds, state)\r\n","        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\r\n","        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state\r\n","        "],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"StuBDokax5cy","executionInfo":{"status":"ok","timestamp":1614081952998,"user_tz":-180,"elapsed":704,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["model = Network().to(dev)"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itYLQBEAyCMO","executionInfo":{"status":"ok","timestamp":1614081976455,"user_tz":-180,"elapsed":593,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"2ba74842-024f-4c1d-e96d-e1d03a9fe5aa"},"source":["X[0:1]"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[20, 14, 10, 10, 18, 27, 21, 15,  9,  9,  8, 21, 25, 11, 14, 24, 12, 21,\n","         24, 11, 14, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Y4-g0MQyHIB","executionInfo":{"status":"ok","timestamp":1614081995251,"user_tz":-180,"elapsed":622,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"532b0d5e-7d2b-4d5b-bbd5-a08abf15f21f"},"source":["model.forward(X[0:1].to(dev))[0].shape"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 50, 28])"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"MhSPUORtqkoc","executionInfo":{"status":"ok","timestamp":1614082069780,"user_tz":-180,"elapsed":617,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["criterion = torch.nn.CrossEntropyLoss()\r\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\r\n","batch_size = 100\r\n","n_epochs = 100"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npiw8BblrmnD","executionInfo":{"status":"ok","timestamp":1614082166201,"user_tz":-180,"elapsed":93851,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"64334629-fb42-4349-9f2c-35d51f70426b"},"source":["for ep in range(10):\r\n","    start = time.time()\r\n","    train_loss = 0.\r\n","    train_passed = 0\r\n","\r\n","    for i in range(int(len(X) / 100)):\r\n","        batch = X[i * 100:(i + 1) * 100]\r\n","        X_batch = batch[:, :-1]\r\n","        Y_batch = batch[:, 1:].flatten()\r\n","\r\n","        optimizer.zero_grad()\r\n","        answers, _ = model.forward(X_batch)\r\n","        answers = answers.view(-1, len(INDEX_TO_CHAR))\r\n","        loss = criterion(answers, Y_batch)\r\n","        train_loss += loss.item()\r\n","\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        train_passed += 1\r\n","\r\n","    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Epoch 0. Time: 9.472, Train loss: 2.213\n","Epoch 1. Time: 9.259, Train loss: 1.958\n","Epoch 2. Time: 9.349, Train loss: 1.891\n","Epoch 3. Time: 9.281, Train loss: 1.850\n","Epoch 4. Time: 9.326, Train loss: 1.817\n","Epoch 5. Time: 9.232, Train loss: 1.787\n","Epoch 6. Time: 9.359, Train loss: 1.760\n","Epoch 7. Time: 9.271, Train loss: 1.735\n","Epoch 8. Time: 9.329, Train loss: 1.713\n","Epoch 9. Time: 9.345, Train loss: 1.694\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7B_DBpCgsmV_"},"source":["## **Практика. Реализуйте код генерации следующей буквы на основе модели.**"]},{"cell_type":"code","metadata":{"id":"cpzrxigFt61B","executionInfo":{"status":"ok","timestamp":1614080960938,"user_tz":-180,"elapsed":588,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["import numpy as np\r\n","import string"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzhRdOIYtAJX","executionInfo":{"status":"ok","timestamp":1614082026075,"user_tz":-180,"elapsed":606,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}}},"source":["def generate_sentence():\r\n","  sentence = ['h', 'e', 'l', 'l', 'o']\r\n","  state = None\r\n","  for i in range(MAX_LEN):\r\n","    X = torch.Tensor([[CHAR_TO_INDEX[sentence[i]]]]).type(torch.long).to(dev)\r\n","    if i == 0:\r\n","      result, state = model.forward(X)\r\n","    else:\r\n","      result, state = model.forward_state(X, state)\r\n","    prediction = result[0, -1, :]\r\n","    index_of_prediction = prediction.argmax()\r\n","    if i >= len(sentence) - 1:\r\n","      if index_of_prediction == 0:\r\n","        break\r\n","      sentence.append(INDEX_TO_CHAR[index_of_prediction])\r\n","\r\n","  print(''.join(sentence))"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rR3NDizdtjBm","executionInfo":{"status":"ok","timestamp":1614082029171,"user_tz":-180,"elapsed":661,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"ef48fa6e-cec7-42cc-85c8-133e0609e433"},"source":["generate_sentence()"],"execution_count":78,"outputs":[{"output_type":"stream","text":["hellohrzz iieexzz iieexzz iieexzz iieexzz iieexzz i\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EZYn30Gvhny","executionInfo":{"status":"ok","timestamp":1614085101065,"user_tz":-180,"elapsed":2812639,"user":{"displayName":"Дмитрий Зеленин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipAVqvGUDyzuPjOfJR-p9zFprHJyE0eR-cN_rS6Q=s64","userId":"06754578418295277569"}},"outputId":"24f51287-0419-4a54-a75d-a6f6e12554a0"},"source":["for ep in range(300):\r\n","    start = time.time()\r\n","    train_loss = 0.\r\n","    train_passed = 0\r\n","\r\n","    for i in range(int(len(X) / 100)):\r\n","        batch = X[i * 100:(i + 1) * 100]\r\n","        X_batch = batch[:, :-1]\r\n","        Y_batch = batch[:, 1:].flatten()\r\n","\r\n","        optimizer.zero_grad()\r\n","        answers, _ = model.forward(X_batch)\r\n","        answers = answers.view(-1, len(INDEX_TO_CHAR))\r\n","        loss = criterion(answers, Y_batch)\r\n","        train_loss += loss.item()\r\n","\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        train_passed += 1\r\n","\r\n","    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\r\n","    generate_sentence()"],"execution_count":82,"outputs":[{"output_type":"stream","text":["Epoch 0. Time: 9.373, Train loss: 1.608\n","hello the the the the the the the the the the the t\n","Epoch 1. Time: 9.368, Train loss: 1.601\n","hello the the the the the the the the the the the t\n","Epoch 2. Time: 9.369, Train loss: 1.594\n","hello the the the the the the the the the the the t\n","Epoch 3. Time: 9.327, Train loss: 1.588\n","hello the the the the the the the the the the the t\n","Epoch 4. Time: 9.391, Train loss: 1.581\n","hello the the the the the the the the the the the t\n","Epoch 5. Time: 9.333, Train loss: 1.576\n","hello the the the the the the the the the the the t\n","Epoch 6. Time: 9.388, Train loss: 1.570\n","hello the the the the the the the the the the the t\n","Epoch 7. Time: 9.367, Train loss: 1.565\n","hello the the the the the the the the the the the t\n","Epoch 8. Time: 9.357, Train loss: 1.559\n","hello the the the the the the the the the the the t\n","Epoch 9. Time: 9.345, Train loss: 1.554\n","hello the the the the the the the the the the the t\n","Epoch 10. Time: 9.381, Train loss: 1.550\n","hello the the the the the the the the the the the t\n","Epoch 11. Time: 9.351, Train loss: 1.545\n","hello the the the the the the the the the the the t\n","Epoch 12. Time: 9.352, Train loss: 1.540\n","hello the the the the the the the the the the the t\n","Epoch 13. Time: 9.335, Train loss: 1.536\n","hello the the the the the the the the the the the t\n","Epoch 14. Time: 9.305, Train loss: 1.531\n","hello the the the the the the the the the the the t\n","Epoch 15. Time: 9.319, Train loss: 1.527\n","hello the the the the the the the the the the the t\n","Epoch 16. Time: 9.417, Train loss: 1.522\n","hello the the the the the the the the the the the t\n","Epoch 17. Time: 9.484, Train loss: 1.518\n","hello the the the the the the the the the the the t\n","Epoch 18. Time: 9.319, Train loss: 1.514\n","hello the the the the the the the the the the the t\n","Epoch 19. Time: 9.422, Train loss: 1.510\n","hello the the the the the the the the the the the t\n","Epoch 20. Time: 9.359, Train loss: 1.505\n","hello the the the the the the the the the the the t\n","Epoch 21. Time: 9.408, Train loss: 1.501\n","hello the the the the the the the the the the the t\n","Epoch 22. Time: 9.322, Train loss: 1.497\n","hello the the the the the the the the the the the t\n","Epoch 23. Time: 9.310, Train loss: 1.493\n","hello the the the the the the the the the the the t\n","Epoch 24. Time: 9.319, Train loss: 1.489\n","hello the the the the the the the the the the the t\n","Epoch 25. Time: 9.335, Train loss: 1.485\n","hello the the the the the the the the the the the t\n","Epoch 26. Time: 9.440, Train loss: 1.482\n","hello the the the the the the the the the the the t\n","Epoch 27. Time: 9.364, Train loss: 1.478\n","hello the the the the the the the the the the the t\n","Epoch 28. Time: 9.361, Train loss: 1.474\n","hello the the the the the the the the the the the t\n","Epoch 29. Time: 9.380, Train loss: 1.470\n","hello the the the the the the the the the the the t\n","Epoch 30. Time: 9.283, Train loss: 1.467\n","hello the the the the the the the the the the the t\n","Epoch 31. Time: 9.288, Train loss: 1.463\n","hello the the the the the the the the the the the t\n","Epoch 32. Time: 9.371, Train loss: 1.460\n","hello the the the the the the the the the the the t\n","Epoch 33. Time: 9.353, Train loss: 1.456\n","hello the the the the the the the the the the the t\n","Epoch 34. Time: 9.385, Train loss: 1.453\n","hellon the the the the the the the the the the the \n","Epoch 35. Time: 9.279, Train loss: 1.449\n","hellon the the the the the the the the the the the \n","Epoch 36. Time: 9.215, Train loss: 1.446\n","hellon the the the the the the the the the the the \n","Epoch 37. Time: 9.440, Train loss: 1.443\n","hellon the the the the the the the the the the the \n","Epoch 38. Time: 9.347, Train loss: 1.439\n","hellon the the the the the the the the the the the \n","Epoch 39. Time: 9.361, Train loss: 1.436\n","hellon the the the the the the the the the the the \n","Epoch 40. Time: 9.286, Train loss: 1.433\n","hellon the the the the the the the the the the the \n","Epoch 41. Time: 9.268, Train loss: 1.430\n","hellon the the the the the the the the the the the \n","Epoch 42. Time: 9.278, Train loss: 1.427\n","hellon the the the the the the the the the the the \n","Epoch 43. Time: 9.293, Train loss: 1.424\n","hellon the the the the the the the the the the the \n","Epoch 44. Time: 9.327, Train loss: 1.421\n","hellon the the the the the the the the the the the \n","Epoch 45. Time: 9.288, Train loss: 1.418\n","hellon the the the the the the the the the the the \n","Epoch 46. Time: 9.258, Train loss: 1.415\n","hellon the the the the the the the the the the the \n","Epoch 47. Time: 9.314, Train loss: 1.412\n","hellon the the the the the the the the the the the \n","Epoch 48. Time: 9.288, Train loss: 1.409\n","hellon the the the the the the the the the the the \n","Epoch 49. Time: 9.350, Train loss: 1.406\n","hellon the the the the the the the the the the the \n","Epoch 50. Time: 9.311, Train loss: 1.403\n","hellon the the the the the the the the the the the \n","Epoch 51. Time: 9.353, Train loss: 1.400\n","hellon the the the the the the the the the the the \n","Epoch 52. Time: 9.420, Train loss: 1.397\n","hellon the the the the the the the the the the the \n","Epoch 53. Time: 9.400, Train loss: 1.394\n","hellon the the soure the the soure the the soure th\n","Epoch 54. Time: 9.353, Train loss: 1.392\n","hellon the soure the soure the soure the soure the \n","Epoch 55. Time: 9.312, Train loss: 1.389\n","hellon the soure the soure the soure the soure the \n","Epoch 56. Time: 9.356, Train loss: 1.386\n","hellon the soure the soure the soure the soure the \n","Epoch 57. Time: 9.330, Train loss: 1.383\n","hellon the soure the soure the soure the soure the \n","Epoch 58. Time: 9.337, Train loss: 1.381\n","hellon the soure the soure the soure the soure the \n","Epoch 59. Time: 9.308, Train loss: 1.378\n","hellon the soure the soure the soure the soure the \n","Epoch 60. Time: 9.344, Train loss: 1.375\n","hellon the soure the soure the soure the soure the \n","Epoch 61. Time: 9.293, Train loss: 1.373\n","hellon the soure the soure the soure the soure the \n","Epoch 62. Time: 9.270, Train loss: 1.370\n","hellon the soure the soure the soure the soure the \n","Epoch 63. Time: 9.336, Train loss: 1.368\n","hellon the so the soure the soure the soure the sou\n","Epoch 64. Time: 9.302, Train loss: 1.365\n","hellon the so the so the so the so the so the so th\n","Epoch 65. Time: 9.311, Train loss: 1.363\n","hellon the so the so the so the so the so the so th\n","Epoch 66. Time: 9.316, Train loss: 1.360\n","hellon the so the so the so the so the so the so th\n","Epoch 67. Time: 9.393, Train loss: 1.357\n","hellon the so the so the so the so the so the so th\n","Epoch 68. Time: 9.450, Train loss: 1.355\n","hellon the so the so the so the so the so the so th\n","Epoch 69. Time: 9.320, Train loss: 1.353\n","hellon the so the so the so the so the so the so th\n","Epoch 70. Time: 9.389, Train loss: 1.350\n","hellon the so the so the so the so the so the so th\n","Epoch 71. Time: 9.319, Train loss: 1.348\n","hellon the so the so the so the so the so the so th\n","Epoch 72. Time: 9.321, Train loss: 1.345\n","hellone the so the so the so the so the so the so t\n","Epoch 73. Time: 9.341, Train loss: 1.343\n","hellone the so the so the so the so the so the so t\n","Epoch 74. Time: 9.309, Train loss: 1.340\n","hellone the so the so the so the so the so the so t\n","Epoch 75. Time: 9.438, Train loss: 1.338\n","hellone the so the so the so the so the so the so t\n","Epoch 76. Time: 9.340, Train loss: 1.336\n","hellone the so the so the so the so the so the so t\n","Epoch 77. Time: 9.288, Train loss: 1.333\n","hellone the so the so the so the so the so the so t\n","Epoch 78. Time: 9.351, Train loss: 1.331\n","hellone the so the so the so the so the so the so t\n","Epoch 79. Time: 9.325, Train loss: 1.329\n","hellone the so the so the so the so the so the so t\n","Epoch 80. Time: 9.278, Train loss: 1.327\n","hellone the so the so the so the so the so the so t\n","Epoch 81. Time: 9.249, Train loss: 1.324\n","hellone the so the so the so the so the so the so t\n","Epoch 82. Time: 9.321, Train loss: 1.322\n","hellone the so the so the so the so the so the so t\n","Epoch 83. Time: 9.345, Train loss: 1.320\n","hellow the so the so the so the so the so the so th\n","Epoch 84. Time: 9.374, Train loss: 1.318\n","hellow the so the so the so the so the so the so th\n","Epoch 85. Time: 9.415, Train loss: 1.316\n","hellow the so the so the so the so the so the so th\n","Epoch 86. Time: 9.353, Train loss: 1.314\n","hellow the so the so the so the so the so the so th\n","Epoch 87. Time: 9.285, Train loss: 1.311\n","hellow the so the so the so the so the so the so th\n","Epoch 88. Time: 9.315, Train loss: 1.309\n","hellow the so the so the so the so the so the so th\n","Epoch 89. Time: 9.318, Train loss: 1.307\n","hellow the so the so the so the so the so the so th\n","Epoch 90. Time: 9.327, Train loss: 1.305\n","hellow the so the so the so the so the so the so th\n","Epoch 91. Time: 9.382, Train loss: 1.303\n","hellow the so the so the so the so the so the so th\n","Epoch 92. Time: 9.285, Train loss: 1.301\n","hellow the so the some the so the some the so the s\n","Epoch 93. Time: 9.299, Train loss: 1.299\n","hellow the so the some the so the some the so the s\n","Epoch 94. Time: 9.256, Train loss: 1.297\n","hellow the so the some the so the some the so the s\n","Epoch 95. Time: 9.375, Train loss: 1.295\n","hellow the so the some the so the some the so the s\n","Epoch 96. Time: 9.332, Train loss: 1.293\n","hellow the so the some the so the some the so the s\n","Epoch 97. Time: 9.309, Train loss: 1.292\n","hellow the so the some the so the some the so the s\n","Epoch 98. Time: 9.345, Train loss: 1.290\n","hellow the so the some the some the some the some t\n","Epoch 99. Time: 9.306, Train loss: 1.288\n","hellow the so the some the some the some the some t\n","Epoch 100. Time: 9.289, Train loss: 1.286\n","hellow the so the some the some the some the some t\n","Epoch 101. Time: 9.308, Train loss: 1.284\n","hellow the so the some the some the some the some t\n","Epoch 102. Time: 9.355, Train loss: 1.282\n","hellow the so the some the some the some the some t\n","Epoch 103. Time: 9.330, Train loss: 1.280\n","hellow the so the some the some the some the some t\n","Epoch 104. Time: 9.348, Train loss: 1.279\n","hellow the so the some the some the some the some t\n","Epoch 105. Time: 9.327, Train loss: 1.277\n","hellow the so the some the some the some the some t\n","Epoch 106. Time: 9.282, Train loss: 1.275\n","hellow the some the some the some the some the some\n","Epoch 107. Time: 9.250, Train loss: 1.273\n","hellow the some the some the some the some the some\n","Epoch 108. Time: 9.328, Train loss: 1.272\n","hellow the some the some the some the some the some\n","Epoch 109. Time: 9.317, Train loss: 1.270\n","hellow the some the some the some the some the some\n","Epoch 110. Time: 9.279, Train loss: 1.268\n","hellow the some the some the some the some the some\n","Epoch 111. Time: 9.300, Train loss: 1.267\n","hellow the some the some the some the some the some\n","Epoch 112. Time: 9.327, Train loss: 1.265\n","hellow the some the some the some the some the some\n","Epoch 113. Time: 9.405, Train loss: 1.263\n","hellow the some the some the some the some the some\n","Epoch 114. Time: 9.332, Train loss: 1.262\n","hellow the some the some the some the some the some\n","Epoch 115. Time: 9.331, Train loss: 1.260\n","hellow the some the some the some the some the some\n","Epoch 116. Time: 9.326, Train loss: 1.259\n","hellow the some the some the some the some the some\n","Epoch 117. Time: 9.340, Train loss: 1.257\n","hellow the some the some the some the some the some\n","Epoch 118. Time: 9.401, Train loss: 1.255\n","hellow the some the some the some the some the some\n","Epoch 119. Time: 9.276, Train loss: 1.254\n","hellow the some the some the some the some the some\n","Epoch 120. Time: 9.471, Train loss: 1.252\n","hellow the some the some the some the some the some\n","Epoch 121. Time: 9.349, Train loss: 1.251\n","hellow the some the some the some the some the some\n","Epoch 122. Time: 9.325, Train loss: 1.249\n","hellow the some the some the some the some the some\n","Epoch 123. Time: 9.326, Train loss: 1.248\n","hellow the some the some the some the some the some\n","Epoch 124. Time: 9.367, Train loss: 1.246\n","hellow the some the some the some the some the some\n","Epoch 125. Time: 9.301, Train loss: 1.245\n","hellow the some the some the some the some the some\n","Epoch 126. Time: 9.330, Train loss: 1.243\n","hellow the some the some the some the some the some\n","Epoch 127. Time: 9.383, Train loss: 1.242\n","hellow the some the some the some the some the some\n","Epoch 128. Time: 9.358, Train loss: 1.241\n","hellow the some the some the some the some the some\n","Epoch 129. Time: 9.328, Train loss: 1.239\n","hellow the some the some the some the some the some\n","Epoch 130. Time: 9.384, Train loss: 1.238\n","hellow the some the some the some the some the some\n","Epoch 131. Time: 9.292, Train loss: 1.236\n","hellow the some the some the some the some the some\n","Epoch 132. Time: 9.408, Train loss: 1.235\n","hellow the some the some the some the some the some\n","Epoch 133. Time: 9.338, Train loss: 1.234\n","hellow the some the some the some the some the some\n","Epoch 134. Time: 9.322, Train loss: 1.232\n","hellow the some the some the some the some the some\n","Epoch 135. Time: 9.312, Train loss: 1.231\n","hellow the some the some the some the some the some\n","Epoch 136. Time: 9.342, Train loss: 1.230\n","hellow the some the some the some the some the some\n","Epoch 137. Time: 9.379, Train loss: 1.228\n","hellow the some the some the some the some the some\n","Epoch 138. Time: 9.284, Train loss: 1.227\n","hellow the some the some the some the some the some\n","Epoch 139. Time: 9.299, Train loss: 1.226\n","hellow the some the some the some the some the some\n","Epoch 140. Time: 9.365, Train loss: 1.224\n","hellow the some the some the some the some the some\n","Epoch 141. Time: 9.365, Train loss: 1.223\n","hellow the some the some the some the some the some\n","Epoch 142. Time: 9.358, Train loss: 1.222\n","hellow the some the some the some the some the some\n","Epoch 143. Time: 9.410, Train loss: 1.221\n","hellow the some the some the some the some the some\n","Epoch 144. Time: 9.285, Train loss: 1.219\n","hellow the some the some the some the some the some\n","Epoch 145. Time: 9.387, Train loss: 1.218\n","hellow the some the some the some the some the some\n","Epoch 146. Time: 9.341, Train loss: 1.217\n","hellow the some the some the some the some the some\n","Epoch 147. Time: 9.404, Train loss: 1.216\n","hellow the some the some the some the some the some\n","Epoch 148. Time: 9.353, Train loss: 1.214\n","hellow the some the some the some the some the some\n","Epoch 149. Time: 9.267, Train loss: 1.213\n","hellow the some the some the some the some the some\n","Epoch 150. Time: 9.344, Train loss: 1.212\n","hellow the some the some the some the some the some\n","Epoch 151. Time: 9.441, Train loss: 1.211\n","hellow the some the some the some the some the some\n","Epoch 152. Time: 9.308, Train loss: 1.210\n","hellow the some the some the some the some the some\n","Epoch 153. Time: 9.293, Train loss: 1.209\n","hellow the some the some the some the some the some\n","Epoch 154. Time: 9.332, Train loss: 1.207\n","hellow the some the some the some the some the some\n","Epoch 155. Time: 9.331, Train loss: 1.206\n","hellow the some the some the some the some the some\n","Epoch 156. Time: 9.270, Train loss: 1.205\n","hellow the some the some the some the some the some\n","Epoch 157. Time: 9.360, Train loss: 1.204\n","hellow the some the some the some the some the some\n","Epoch 158. Time: 9.301, Train loss: 1.203\n","hellow the some the some the some the some the some\n","Epoch 159. Time: 9.378, Train loss: 1.202\n","hellow the some the some the some the some the some\n","Epoch 160. Time: 9.381, Train loss: 1.201\n","hellow the some the some the some the some the some\n","Epoch 161. Time: 9.279, Train loss: 1.200\n","hellow the some the some the some the some the some\n","Epoch 162. Time: 9.308, Train loss: 1.199\n","hellow the some the some the some the some the some\n","Epoch 163. Time: 9.361, Train loss: 1.197\n","hellow the some the some the some the some the some\n","Epoch 164. Time: 9.363, Train loss: 1.196\n","hellow what i dont the some the some the some the s\n","Epoch 165. Time: 9.437, Train loss: 1.195\n","hellow what i dont the some the some the some the s\n","Epoch 166. Time: 9.363, Train loss: 1.194\n","hellow what i dont the some the some the some the s\n","Epoch 167. Time: 9.314, Train loss: 1.193\n","hellow what i dont the some the some the some the s\n","Epoch 168. Time: 9.333, Train loss: 1.192\n","hellow what i dont the some the some the some the s\n","Epoch 169. Time: 9.394, Train loss: 1.191\n","hellow what i dont the some the some the some the s\n","Epoch 170. Time: 9.479, Train loss: 1.190\n","hellow what i dont the some the some the some the s\n","Epoch 171. Time: 9.396, Train loss: 1.189\n","hellow what i dont the some the some the some the s\n","Epoch 172. Time: 9.411, Train loss: 1.188\n","hellow what i dont the some the some the some the s\n","Epoch 173. Time: 9.342, Train loss: 1.187\n","hellow what i dont the some the some the some the s\n","Epoch 174. Time: 9.354, Train loss: 1.186\n","hellow what i dont the some the some the some the s\n","Epoch 175. Time: 9.308, Train loss: 1.185\n","hellow what i dont the some the some the some the s\n","Epoch 176. Time: 9.402, Train loss: 1.184\n","hellow what i dont the some the some to the some th\n","Epoch 177. Time: 9.383, Train loss: 1.183\n","hellow what i dont the some to the some to the some\n","Epoch 178. Time: 9.337, Train loss: 1.182\n","hellow what i dont the some to the some to the some\n","Epoch 179. Time: 9.280, Train loss: 1.181\n","hellow what i dont the some to the some to the some\n","Epoch 180. Time: 9.362, Train loss: 1.180\n","hellow what i dont the see the something the some t\n","Epoch 181. Time: 9.296, Train loss: 1.179\n","hellow what i dont the see the something the some t\n","Epoch 182. Time: 9.354, Train loss: 1.178\n","hellow what i dont the see the something the some t\n","Epoch 183. Time: 9.444, Train loss: 1.178\n","hellow what i dont the see the something the some t\n","Epoch 184. Time: 9.356, Train loss: 1.177\n","hellow what i dont the see the something the some t\n","Epoch 185. Time: 9.399, Train loss: 1.176\n","hellow what i dont the see the something the see th\n","Epoch 186. Time: 9.301, Train loss: 1.175\n","hellow what i dont the see the something the see th\n","Epoch 187. Time: 9.307, Train loss: 1.174\n","hellow what i dont the see the something the see th\n","Epoch 188. Time: 9.298, Train loss: 1.173\n","hellow what i dont the see the see the see the see \n","Epoch 189. Time: 9.365, Train loss: 1.172\n","hellow what i dont the see the see the see the see \n","Epoch 190. Time: 9.327, Train loss: 1.171\n","hellow what i dont was the see the see the see the \n","Epoch 191. Time: 9.353, Train loss: 1.170\n","hellow what i dont was the see the see the see the \n","Epoch 192. Time: 9.363, Train loss: 1.169\n","hellow what i dont was the see the see the see the \n","Epoch 193. Time: 9.365, Train loss: 1.169\n","hellow what i dont was the see the see the see the \n","Epoch 194. Time: 9.306, Train loss: 1.168\n","hellow what i dont was the see the see the see the \n","Epoch 195. Time: 9.400, Train loss: 1.167\n","hellow what i dont was the see the see the see the \n","Epoch 196. Time: 9.271, Train loss: 1.166\n","hellow what i dont was a like the see the see the s\n","Epoch 197. Time: 9.479, Train loss: 1.165\n","hellow what i dont was a like the see the see the s\n","Epoch 198. Time: 9.323, Train loss: 1.164\n","hellow what i dont was a like the see the see the s\n","Epoch 199. Time: 9.332, Train loss: 1.163\n","hellow what i dont was a like the see the see the s\n","Epoch 200. Time: 9.335, Train loss: 1.163\n","hellow what i dont was a like the see the see the s\n","Epoch 201. Time: 9.354, Train loss: 1.162\n","hellow what i dont was a like the see the see the s\n","Epoch 202. Time: 9.336, Train loss: 1.161\n","hellow what i dont was a like the see the see the s\n","Epoch 203. Time: 9.402, Train loss: 1.160\n","hellow what i dont was a like the see the see the s\n","Epoch 204. Time: 9.375, Train loss: 1.159\n","hellow what i dont was a like the see the see the s\n","Epoch 205. Time: 9.376, Train loss: 1.158\n","hellow what i dont was a like the see the see the s\n","Epoch 206. Time: 9.350, Train loss: 1.158\n","hello i was a like the see the see the see the see \n","Epoch 207. Time: 9.327, Train loss: 1.157\n","hello i was a like the see the see the see the see \n","Epoch 208. Time: 9.358, Train loss: 1.156\n","hello i was a like the see the see the see the see \n","Epoch 209. Time: 9.352, Train loss: 1.155\n","hello i was a like the see the see the see the see \n","Epoch 210. Time: 9.317, Train loss: 1.154\n","hello i was a like the see the see the see the see \n","Epoch 211. Time: 9.340, Train loss: 1.154\n","hello i was a like the see the see the see the see \n","Epoch 212. Time: 9.371, Train loss: 1.153\n","hello i was a like the see the see the see the see \n","Epoch 213. Time: 9.386, Train loss: 1.152\n","hello i was a like the see the see the see the see \n","Epoch 214. Time: 9.292, Train loss: 1.151\n","hello i was a like the see the see the see the see \n","Epoch 215. Time: 9.284, Train loss: 1.151\n","hello i was a like the see the see the see the see \n","Epoch 216. Time: 9.399, Train loss: 1.150\n","hello i was a like the see the see the see the see \n","Epoch 217. Time: 9.336, Train loss: 1.149\n","hello i was a like the see the see the see the see \n","Epoch 218. Time: 9.369, Train loss: 1.148\n","hello i was a like the see the see the see the see \n","Epoch 219. Time: 9.410, Train loss: 1.148\n","hello i was a like the see the see the see the see \n","Epoch 220. Time: 9.312, Train loss: 1.147\n","hello i was a like the see the see the see the see \n","Epoch 221. Time: 9.321, Train loss: 1.146\n","hello i was a like the see the see the see the see \n","Epoch 222. Time: 9.367, Train loss: 1.145\n","hello i was a like the see the see the see the see \n","Epoch 223. Time: 9.341, Train loss: 1.145\n","hello i was a like the see the see the see the see \n","Epoch 224. Time: 9.357, Train loss: 1.144\n","hello i was a like the see the see the see the see \n","Epoch 225. Time: 9.352, Train loss: 1.143\n","hello i was a like the see the see the see the see \n","Epoch 226. Time: 9.342, Train loss: 1.142\n","hello i was a like the see the see the see the see \n","Epoch 227. Time: 9.413, Train loss: 1.142\n","hello i was a like the see the see the see the see \n","Epoch 228. Time: 9.313, Train loss: 1.141\n","hello i was a like the see the see the see the see \n","Epoch 229. Time: 9.337, Train loss: 1.140\n","hello i was a like the see the see the see the see \n","Epoch 230. Time: 9.359, Train loss: 1.140\n","hello i was a like the see the see the see the see \n","Epoch 231. Time: 9.354, Train loss: 1.139\n","hello i was a like the see the see the see the see \n","Epoch 232. Time: 9.417, Train loss: 1.138\n","hello i was a like the see the see the see the see \n","Epoch 233. Time: 9.290, Train loss: 1.138\n","hello i was a like the see the see the see the see \n","Epoch 234. Time: 9.352, Train loss: 1.137\n","hello i was a like the see the see the see the see \n","Epoch 235. Time: 9.356, Train loss: 1.136\n","hello i was a like the see the see the see the see \n","Epoch 236. Time: 9.297, Train loss: 1.135\n","hello i was a like the see the see the see the see \n","Epoch 237. Time: 9.412, Train loss: 1.135\n","hello i was a like the see the see the see the see \n","Epoch 238. Time: 9.384, Train loss: 1.134\n","hello i was a like the see the see the see the see \n","Epoch 239. Time: 9.324, Train loss: 1.133\n","hello i want the see the see the see the see the se\n","Epoch 240. Time: 9.356, Train loss: 1.133\n","hello i want the see the see the see the see the se\n","Epoch 241. Time: 9.323, Train loss: 1.132\n","hello i want the see the see the see the see the se\n","Epoch 242. Time: 9.347, Train loss: 1.131\n","hello i want the see the see the see the see the se\n","Epoch 243. Time: 9.369, Train loss: 1.131\n","hello i want the see the see the see the see the se\n","Epoch 244. Time: 9.420, Train loss: 1.130\n","hello i want the see the see the see the see the se\n","Epoch 245. Time: 9.330, Train loss: 1.130\n","hello i want the see the see the see the see the se\n","Epoch 246. Time: 9.327, Train loss: 1.129\n","hello i want the see the see the see the see the se\n","Epoch 247. Time: 9.325, Train loss: 1.128\n","hello i want the see the see the see the see the se\n","Epoch 248. Time: 9.367, Train loss: 1.128\n","hello i want the see the see the see the see the se\n","Epoch 249. Time: 9.368, Train loss: 1.127\n","hello i want the see the see the see the see the se\n","Epoch 250. Time: 9.397, Train loss: 1.126\n","hello i want the see the see the see the see the se\n","Epoch 251. Time: 9.470, Train loss: 1.126\n","hello i want the see the see the see the see the se\n","Epoch 252. Time: 9.294, Train loss: 1.125\n","hello i want the see the see the see the see the se\n","Epoch 253. Time: 9.292, Train loss: 1.124\n","hello i want the see the see the see the see the se\n","Epoch 254. Time: 9.314, Train loss: 1.124\n","hello i want the see the see the see the see the se\n","Epoch 255. Time: 9.317, Train loss: 1.123\n","hello i want the see the see the see the see the se\n","Epoch 256. Time: 9.293, Train loss: 1.123\n","hello i want the see the see the see the see the se\n","Epoch 257. Time: 9.300, Train loss: 1.122\n","hello i want the see the see the see the see the se\n","Epoch 258. Time: 9.337, Train loss: 1.121\n","hello i want the see the see the see the see the se\n","Epoch 259. Time: 9.313, Train loss: 1.121\n","hello i want the see the see the see the see the se\n","Epoch 260. Time: 9.378, Train loss: 1.120\n","hello i want the see the see the see the see the se\n","Epoch 261. Time: 9.372, Train loss: 1.120\n","hello i want the see the see the see the see the se\n","Epoch 262. Time: 9.372, Train loss: 1.119\n","hello i want the see the see the see the see the se\n","Epoch 263. Time: 9.284, Train loss: 1.118\n","hello i want the see the see the see the see the se\n","Epoch 264. Time: 9.353, Train loss: 1.118\n","hello i want the see the see the see the see the se\n","Epoch 265. Time: 9.435, Train loss: 1.117\n","hello i want the see the see the see the see the se\n","Epoch 266. Time: 9.268, Train loss: 1.117\n","hello i want the see the see the see the see the se\n","Epoch 267. Time: 9.313, Train loss: 1.116\n","hello i want the see the see the see the see the se\n","Epoch 268. Time: 9.383, Train loss: 1.115\n","hello i want the see the see the see the see the se\n","Epoch 269. Time: 9.351, Train loss: 1.115\n","hello i want the see the see the see the see the se\n","Epoch 270. Time: 9.417, Train loss: 1.114\n","hello i want the see the see the see the see the se\n","Epoch 271. Time: 9.566, Train loss: 1.114\n","hello i want the see the see the see the see the se\n","Epoch 272. Time: 9.325, Train loss: 1.113\n","hello i want the see the see the see the see the se\n","Epoch 273. Time: 9.408, Train loss: 1.113\n","hello i want the see the see the see the see the se\n","Epoch 274. Time: 9.585, Train loss: 1.112\n","hello i want the see the see the see the see the se\n","Epoch 275. Time: 9.388, Train loss: 1.111\n","hello i want the see the see the see the see the se\n","Epoch 276. Time: 9.366, Train loss: 1.111\n","hello i want the see the see the see the see the se\n","Epoch 277. Time: 9.392, Train loss: 1.110\n","hello i want the see the see the see the see the se\n","Epoch 278. Time: 9.408, Train loss: 1.110\n","hello i want the see the see the see the see the se\n","Epoch 279. Time: 9.374, Train loss: 1.109\n","hello i want the see the see the see the see the se\n","Epoch 280. Time: 9.359, Train loss: 1.109\n","hello i want the see the see the see the see the se\n","Epoch 281. Time: 9.437, Train loss: 1.108\n","hello i want the see the see the see the see the se\n","Epoch 282. Time: 9.394, Train loss: 1.108\n","hello i want the see the see the see the see the se\n","Epoch 283. Time: 9.410, Train loss: 1.107\n","hello i want the see the see the see the see the se\n","Epoch 284. Time: 9.394, Train loss: 1.106\n","hello i want the see the see the see the see the se\n","Epoch 285. Time: 9.515, Train loss: 1.106\n","hello i want the see the see the see the see the se\n","Epoch 286. Time: 9.416, Train loss: 1.105\n","hello i want the see the see the see the see the se\n","Epoch 287. Time: 9.507, Train loss: 1.105\n","hello i want the see the see the see the see the se\n","Epoch 288. Time: 9.468, Train loss: 1.104\n","hello i want the see the see the see the see the se\n","Epoch 289. Time: 9.343, Train loss: 1.104\n","hello i want the see the see the see the see the se\n","Epoch 290. Time: 9.425, Train loss: 1.103\n","hello i want the see the see the see the see the se\n","Epoch 291. Time: 9.511, Train loss: 1.103\n","hello i want the see the see the see the see the se\n","Epoch 292. Time: 9.422, Train loss: 1.102\n","hello i want the see the see the see the see the se\n","Epoch 293. Time: 9.450, Train loss: 1.102\n","hello i want the see the see the see the see the se\n","Epoch 294. Time: 9.408, Train loss: 1.101\n","hello i want the see the see the see the see the se\n","Epoch 295. Time: 9.427, Train loss: 1.101\n","hello i want the see the see the see the see the se\n","Epoch 296. Time: 9.415, Train loss: 1.100\n","hello i want the see the see the see the see the se\n","Epoch 297. Time: 9.395, Train loss: 1.100\n","hello i want the see the see the see the see the se\n","Epoch 298. Time: 9.425, Train loss: 1.099\n","hello i want the see the see the see the see the se\n","Epoch 299. Time: 9.388, Train loss: 1.099\n","hello i want the see the see the see the see the se\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P3Pn-dRPvhpd"},"source":[""],"execution_count":null,"outputs":[]}]}